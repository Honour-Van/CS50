# Python爬虫练习

进入国家统计局[2020年统计用区划代码和城乡划分](http://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/2020/index.html)。以此为起点，自动爬取各省市自治区相应统计单位一直到最基层为止，以此为基础，对中国基层行政单位的名称进行统计分析。这样的分析此前已经存在，比如网易数读的文章《[我们分析了67万个村名，找到了中国地名的秘密](https://xw.qq.com/cmsid/20181204A18Y2Q00?f=dc)》。

## 数据获取

爬取后的输出格式如下图所示(没有到最基层，以此类推)。

每一级统计单位相对于上一级用\t缩进，最高一级为省市自治区。每个统计单位前为统计用区划代码。

最高一级的统计用区划代码为该区域前两位，后面依次补零。

![image-20210421183006475](.\assets\image-20210421183006475.png)

最基层有“城乡分类代码”，将该代码放到名称之后，形如：“110112005001天赐良园社区居委会111”，其中的111为“城乡分类代码”。

将分层的文本数据储存到`StatData.txt`中

## 数据统计与分析

### task 1

“城乡分类代码”含义如下：“111表示主城区，112表示城乡结合区，121表示镇中心区，122表示镇乡结合区，123表示特殊区域，210表示乡中心区，220表示村庄”。分别统计各分类最基层统计单位数量格式如下：

![image-20210421190029659](.\assets\image-20210421190029659.png)

省市名称顺序自定，间距自定或采用Tab。



### task 2

分别针对“内蒙古自治区”和“河南省”含有“村委会”的最基层统计单位，统计去除“村委会”后，最常用字前100个，观察其异同，输出按字的频率又高到低顺序输出；

### task 3

根据文后附属的姓氏排行，统计带有不同姓氏的地名数量。注意：仅统计第一个字，仅统计最低两个层次；输出按文后给出的姓氏顺序，格式为每行前一个字符串为姓氏，中间以Tab隔开，后面为该形式的地名数

```sql
01李 02王 03张 04刘 05陈 06杨 07赵 08黄 09周 10吴
11徐 12孙 13胡 14朱 15高 16林 17何 18郭 19马 20罗
21梁 22宋 23郑 24谢 25韩 26唐 27冯 28于 29董 30萧
31程 32曹 33袁 34邓 35许 36傅 37沈 38曾 39彭 40吕
41苏 42卢 43蒋 44蔡 45贾 46丁 47魏 48薛 49叶 50阎
51余 52潘 53杜 54戴 55夏 56钟 57汪 58田 59任 60姜
61范 62方 63石 64姚 65谭 66廖 67邹 68熊 69金 70陆
71郝 72孔 73白 74崔 75康 76毛 77邱 78秦 79江 80史
81顾 82侯 83邵 84孟 85龙 86万 87段 88漕 89钱 90汤
91尹 92黎 93易 94常 95武 96乔 97贺 98赖 99龚 100文
```

将上次三个小题的数据输出到`ComputingData.txt`



## 补充说明

可以使用多线程加速运行时间

考虑

## 代码实现
### 爬虫单元

`parco.py`中给出一个函数，每一个链接的调用返回一个`名称:链接`的字典

由于要输出，所以我们考虑把层数加进去。这样就可以有机地调整输出

小问题：
1. 诸如：
```
<a href="11.html">北京市<br/></a>
```
的网页，不能直接从string属性当中获得标签对应的文本。

解决方法：删除不配对标签 https://blog.csdn.net/u012587107/article/details/80543977



得到省级的字典如下：

```json
{'北京市': '11.html', '天津市': '12.html', '河北省': '13.html', '山西省': '14.html', '内蒙古自治区': '15.html', '辽宁省': '21.html', '吉林省': '22.html', '黑龙江省': '23.html', '上海市': '31.html', '江苏省': '32.html', '浙江省': '33.html', '安徽省': '34.html', '福建省': '35.html', '江西省': '36.html', '山东省': '37.html', '河南省': '41.html', '湖北省': '42.html', '湖南省': '43.html', '广东省': '44.html', '广西壮族自治区': '45.html', '海南省': '46.html', '重庆市': '50.html', '四川省': '51.html', '贵州省': '52.html', '云南省': '53.html', '西藏自治区': '54.html', '陕西省': '61.html', '甘肃省': '62.html', '青海省': '63.html', '宁夏回族自治区': '64.html', '新疆维吾尔自治区': '65.html'}
```



### 分层调用
baselink开始，调用三层或四层，就可以得到想要的数据结果。

每一层调用：

1. 先输出当前层对应的名称：将当前dict的value补全0输出
2. 开始遍历
   1. 如果是最里层：直接输出内容
   2. 如果是外层：递归遍历

第二层要去重：

```python
for key, val in res.items():
	if key.isdigit():
		name = key
	else:
		name += key
```



深层反爬？出现问题

```
                130671000000保定高新技术产业开发区
                130672000000保定白沟新城
                130681000000涿州市
                130682000000定州市
                130683000000安国市
                130684000000高碑店市
        130700000000张家口市
        130800000000承德市
        130900000000沧州市
        131000000000廊坊市
        131100000000衡水市
140000000000山西省
150000000000内蒙古自治区
210000000000辽宁省
220000000000吉林省
230000000000黑龙江省
310000000000上海市
320000000000江苏省
330000000000浙江省
```

并非。是因为后面的省市都换用了table结构。随后我们对嗅探函数parco.collect进行扩充



没有认识到find_all返回的resultset是一个列表结构，吃了不少亏


#### 异常处理
```python
while True:
	try:
		# 超时时间为1秒
		print('1')
		response = requests.get(url, timeout=1)
		print('2')
		response.encoding = "GBK" # 必须要用这个？
		print('3')
		if response.status_code == 200:
		print('4')
		return BeautifulSoup(response.text, "html.parser")
```
我们在输出中发现了诸如连续的1。后来确定，达到timeout之后确实会触发异常。


同时由于数据量极大，我们简单对比了几个平台之下的速度（1分钟）：
powershell：2959
vscode：2187
wsl linux:2648
cmd: 3126